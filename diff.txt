diff --git a/api/proto/banyandb/measure/v1/query.proto b/api/proto/banyandb/measure/v1/query.proto
index a3d10adb..0568202d 100644
--- a/api/proto/banyandb/measure/v1/query.proto
+++ b/api/proto/banyandb/measure/v1/query.proto
@@ -138,6 +138,8 @@ message QueryRequest {
   bool trace = 13;
   // stages is used to specify the stage of the data points in the lifecycle
   repeated string stages = 14;
-  // rewriteAggTopNResult will rewrite agg result to raw data
+  // rewrite_agg_top_n_result will rewrite agg result to raw data
   bool rewrite_agg_top_n_result = 15;
+  // agg_return_partial when true asks data nodes to return aggregation partials (for reduce at liaison)
+  bool agg_return_partial = 16;
 }
diff --git a/banyand/measure/topn_post_processor.go b/banyand/measure/topn_post_processor.go
index 4d219d73..a84e8499 100644
--- a/banyand/measure/topn_post_processor.go
+++ b/banyand/measure/topn_post_processor.go
@@ -70,9 +70,9 @@ func (taggr *topNPostProcessor) Len() int {
 // while for ASC, a max heap has to be built.
 func (taggr *topNPostProcessor) Less(i, j int) bool {
 	if taggr.sort == modelv1.Sort_SORT_DESC {
-		return taggr.items[i].int64Func.Val() < taggr.items[j].int64Func.Val()
+		return taggr.items[i].mapFunc.Val() < taggr.items[j].mapFunc.Val()
 	}
-	return taggr.items[i].int64Func.Val() > taggr.items[j].int64Func.Val()
+	return taggr.items[i].mapFunc.Val() > taggr.items[j].mapFunc.Val()
 }
 
 func (taggr *topNPostProcessor) Swap(i, j int) {
@@ -100,8 +100,8 @@ func (taggr *topNPostProcessor) Pop() any {
 
 func (taggr *topNPostProcessor) tryEnqueue(key string, item *topNAggregatorItem) {
 	if lowest := taggr.items[0]; lowest != nil {
-		shouldReplace := (taggr.sort == modelv1.Sort_SORT_DESC && lowest.int64Func.Val() < item.int64Func.Val()) ||
-			(taggr.sort != modelv1.Sort_SORT_DESC && lowest.int64Func.Val() > item.int64Func.Val())
+		shouldReplace := (taggr.sort == modelv1.Sort_SORT_DESC && lowest.mapFunc.Val() < item.mapFunc.Val()) ||
+			(taggr.sort != modelv1.Sort_SORT_DESC && lowest.mapFunc.Val() > item.mapFunc.Val())
 
 		if shouldReplace {
 			delete(taggr.cache, lowest.key)
@@ -116,12 +116,12 @@ func (taggr *topNPostProcessor) tryEnqueue(key string, item *topNAggregatorItem)
 var _ flow.Element = (*topNAggregatorItem)(nil)
 
 type topNAggregatorItem struct {
-	int64Func aggregation.Func[int64]
-	key       string
-	values    pbv1.EntityValues
-	val       int64
-	version   int64
-	index     int
+	mapFunc aggregation.Map[int64]
+	key     string
+	values  pbv1.EntityValues
+	val     int64
+	version int64
+	index   int
 }
 
 func (n *topNAggregatorItem) GetTags(tagNames []string) []*modelv1.Tag {
@@ -245,18 +245,18 @@ func (taggr *topNPostProcessor) Flush() ([]*topNAggregatorItem, error) {
 		for _, timeline := range taggr.timelines {
 			for _, item := range timeline.items {
 				if exist, found := taggr.cache[item.key]; found {
-					exist.int64Func.In(item.val)
+					exist.mapFunc.In(item.val)
 					heap.Fix(taggr, exist.index)
 					continue
 				}
 
-				aggrFunc, err := aggregation.NewFunc[int64](taggr.aggrFunc)
+				mapFunc, err := aggregation.NewMap[int64](taggr.aggrFunc)
 				if err != nil {
 					return nil, err
 				}
 
-				item.int64Func = aggrFunc
-				item.int64Func.In(item.val)
+				item.mapFunc = mapFunc
+				item.mapFunc.In(item.val)
 
 				if taggr.Len() < int(taggr.topN) {
 					taggr.cache[item.key] = item
@@ -300,7 +300,7 @@ func (taggr *topNPostProcessor) valWithAggregation(tagNames []string) ([]*measur
 			Entity: item.GetTags(tagNames),
 			Value: &modelv1.FieldValue{
 				Value: &modelv1.FieldValue_Int{
-					Int: &modelv1.Int{Value: item.int64Func.Val()},
+					Int: &modelv1.Int{Value: item.mapFunc.Val()},
 				},
 			},
 		}
diff --git a/docs/api-reference.md b/docs/api-reference.md
index 99410e72..8c96a41b 100644
--- a/docs/api-reference.md
+++ b/docs/api-reference.md
@@ -986,7 +986,8 @@ QueryRequest is the request contract for query.
 | order_by | [banyandb.model.v1.QueryOrder](#banyandb-model-v1-QueryOrder) |  | order_by is given to specify the sort for a tag. |
 | trace | [bool](#bool) |  | trace is used to enable trace for the query |
 | stages | [string](#string) | repeated | stages is used to specify the stage of the data points in the lifecycle |
-| rewrite_agg_top_n_result | [bool](#bool) |  | rewriteAggTopNResult will rewrite agg result to raw data |
+| rewrite_agg_top_n_result | [bool](#bool) |  | rewrite_agg_top_n_result will rewrite agg result to raw data |
+| agg_return_partial | [bool](#bool) |  | agg_return_partial when true asks data nodes to return aggregation partials (for reduce at liaison) |
 
 
 
diff --git a/pkg/query/aggregation/aggregation.go b/pkg/query/aggregation/aggregation.go
index a581e670..c520a39d 100644
--- a/pkg/query/aggregation/aggregation.go
+++ b/pkg/query/aggregation/aggregation.go
@@ -31,10 +31,26 @@ var (
 	errUnSupportedFieldType = errors.New("unsupported field type")
 )
 
-// Func supports aggregation operations.
-type Func[N Number] interface {
+// Partial represents the intermediate result of a Map phase.
+// For most functions only Value is meaningful; for MEAN both Value (sum) and Count are used.
+type Partial[N Number] struct {
+	Value N
+	Count N
+}
+
+// Map accumulates raw values and produces aggregation results.
+// It serves as the local accumulator for raw data points.
+type Map[N Number] interface {
 	In(N)
 	Val() N
+	Partial() Partial[N]
+	Reset()
+}
+
+// Reduce combines intermediate results from Map phases into a final value.
+type Reduce[N Number] interface {
+	Combine(Partial[N])
+	Val() N
 	Reset()
 }
 
@@ -43,9 +59,9 @@ type Number interface {
 	~int64 | ~float64
 }
 
-// NewFunc returns a aggregation function based on function type.
-func NewFunc[N Number](af modelv1.AggregationFunction) (Func[N], error) {
-	var result Func[N]
+// NewMap returns a Map aggregation function for the given type.
+func NewMap[N Number](af modelv1.AggregationFunction) (Map[N], error) {
+	var result Map[N]
 	switch af {
 	case modelv1.AggregationFunction_AGGREGATION_FUNCTION_MEAN:
 		result = &meanFunc[N]{zero: zero[N]()}
@@ -64,6 +80,27 @@ func NewFunc[N Number](af modelv1.AggregationFunction) (Func[N], error) {
 	return result, nil
 }
 
+// NewReduce returns a Reduce aggregation function for the given type.
+func NewReduce[N Number](af modelv1.AggregationFunction) (Reduce[N], error) {
+	var result Reduce[N]
+	switch af {
+	case modelv1.AggregationFunction_AGGREGATION_FUNCTION_MEAN:
+		result = &meanReduceFunc[N]{zero: zero[N]()}
+	case modelv1.AggregationFunction_AGGREGATION_FUNCTION_COUNT:
+		result = &countReduceFunc[N]{zero: zero[N]()}
+	case modelv1.AggregationFunction_AGGREGATION_FUNCTION_MAX:
+		result = &maxReduceFunc[N]{min: minOf[N]()}
+	case modelv1.AggregationFunction_AGGREGATION_FUNCTION_MIN:
+		result = &minReduceFunc[N]{max: maxOf[N]()}
+	case modelv1.AggregationFunction_AGGREGATION_FUNCTION_SUM:
+		result = &sumReduceFunc[N]{zero: zero[N]()}
+	default:
+		return nil, errors.WithMessagef(errUnknownFunc, "unknown function:%s", modelv1.AggregationFunction_name[int32(af)])
+	}
+	result.Reset()
+	return result, nil
+}
+
 // FromFieldValue transforms modelv1.FieldValue to Number.
 func FromFieldValue[N Number](fieldValue *modelv1.FieldValue) (N, error) {
 	switch fieldValue.GetValue().(type) {
@@ -86,6 +123,49 @@ func ToFieldValue[N Number](value N) (*modelv1.FieldValue, error) {
 	return nil, errUnSupportedFieldType
 }
 
+// PartialToFieldValues converts a Partial to field values for wire transport.
+// For MEAN it returns two values (Value/sum first, Count second); for others one value.
+func PartialToFieldValues[N Number](af modelv1.AggregationFunction, p Partial[N]) ([]*modelv1.FieldValue, error) {
+	if af == modelv1.AggregationFunction_AGGREGATION_FUNCTION_MEAN {
+		vFv, err := ToFieldValue(p.Value)
+		if err != nil {
+			return nil, err
+		}
+		cFv, err := ToFieldValue(p.Count)
+		if err != nil {
+			return nil, err
+		}
+		return []*modelv1.FieldValue{vFv, cFv}, nil
+	}
+	vFv, err := ToFieldValue(p.Value)
+	if err != nil {
+		return nil, err
+	}
+	return []*modelv1.FieldValue{vFv}, nil
+}
+
+// FieldValuesToPartial converts field values from wire transport to a Partial.
+// For MEAN expects two values (sum, count); for others one value (Count will be zero).
+func FieldValuesToPartial[N Number](af modelv1.AggregationFunction, fvs []*modelv1.FieldValue) (Partial[N], error) {
+	var p Partial[N]
+	if len(fvs) == 0 {
+		return p, nil
+	}
+	v, err := FromFieldValue[N](fvs[0])
+	if err != nil {
+		return p, err
+	}
+	p.Value = v
+	if af == modelv1.AggregationFunction_AGGREGATION_FUNCTION_MEAN && len(fvs) >= 2 {
+		c, err := FromFieldValue[N](fvs[1])
+		if err != nil {
+			return p, err
+		}
+		p.Count = c
+	}
+	return p, nil
+}
+
 func minOf[N Number]() (r N) {
 	switch x := any(&r).(type) {
 	case *int64:
diff --git a/pkg/query/aggregation/function.go b/pkg/query/aggregation/function.go
index faa019d9..24bd0f9e 100644
--- a/pkg/query/aggregation/function.go
+++ b/pkg/query/aggregation/function.go
@@ -39,11 +39,42 @@ func (m meanFunc[N]) Val() N {
 	return v
 }
 
+func (m meanFunc[N]) Partial() Partial[N] {
+	return Partial[N]{Value: m.sum, Count: m.count}
+}
+
 func (m *meanFunc[N]) Reset() {
 	m.sum = m.zero
 	m.count = m.zero
 }
 
+type meanReduceFunc[N Number] struct {
+	sum   N
+	count N
+	zero  N
+}
+
+func (m *meanReduceFunc[N]) Combine(p Partial[N]) {
+	m.sum += p.Value
+	m.count += p.Count
+}
+
+func (m meanReduceFunc[N]) Val() N {
+	if m.count == m.zero {
+		return m.zero
+	}
+	v := m.sum / m.count
+	if v < 1 {
+		return 1
+	}
+	return v
+}
+
+func (m *meanReduceFunc[N]) Reset() {
+	m.sum = m.zero
+	m.count = m.zero
+}
+
 type countFunc[N Number] struct {
 	count N
 	zero  N
@@ -57,10 +88,31 @@ func (c countFunc[N]) Val() N {
 	return c.count
 }
 
+func (c countFunc[N]) Partial() Partial[N] {
+	return Partial[N]{Value: c.count, Count: c.zero}
+}
+
 func (c *countFunc[N]) Reset() {
 	c.count = c.zero
 }
 
+type countReduceFunc[N Number] struct {
+	sum  N
+	zero N
+}
+
+func (c *countReduceFunc[N]) Combine(p Partial[N]) {
+	c.sum += p.Value
+}
+
+func (c countReduceFunc[N]) Val() N {
+	return c.sum
+}
+
+func (c *countReduceFunc[N]) Reset() {
+	c.sum = c.zero
+}
+
 type sumFunc[N Number] struct {
 	sum  N
 	zero N
@@ -74,10 +126,31 @@ func (s sumFunc[N]) Val() N {
 	return s.sum
 }
 
+func (s sumFunc[N]) Partial() Partial[N] {
+	return Partial[N]{Value: s.sum, Count: s.zero}
+}
+
 func (s *sumFunc[N]) Reset() {
 	s.sum = s.zero
 }
 
+type sumReduceFunc[N Number] struct {
+	sum  N
+	zero N
+}
+
+func (s *sumReduceFunc[N]) Combine(p Partial[N]) {
+	s.sum += p.Value
+}
+
+func (s sumReduceFunc[N]) Val() N {
+	return s.sum
+}
+
+func (s *sumReduceFunc[N]) Reset() {
+	s.sum = s.zero
+}
+
 type maxFunc[N Number] struct {
 	val N
 	min N
@@ -93,10 +166,33 @@ func (m maxFunc[N]) Val() N {
 	return m.val
 }
 
+func (m maxFunc[N]) Partial() Partial[N] {
+	return Partial[N]{Value: m.val, Count: m.min}
+}
+
 func (m *maxFunc[N]) Reset() {
 	m.val = m.min
 }
 
+type maxReduceFunc[N Number] struct {
+	val N
+	min N
+}
+
+func (m *maxReduceFunc[N]) Combine(p Partial[N]) {
+	if p.Value > m.val {
+		m.val = p.Value
+	}
+}
+
+func (m maxReduceFunc[N]) Val() N {
+	return m.val
+}
+
+func (m *maxReduceFunc[N]) Reset() {
+	m.val = m.min
+}
+
 type minFunc[N Number] struct {
 	val N
 	max N
@@ -112,6 +208,29 @@ func (m minFunc[N]) Val() N {
 	return m.val
 }
 
+func (m minFunc[N]) Partial() Partial[N] {
+	return Partial[N]{Value: m.val, Count: m.max}
+}
+
 func (m *minFunc[N]) Reset() {
 	m.val = m.max
 }
+
+type minReduceFunc[N Number] struct {
+	val N
+	max N
+}
+
+func (m *minReduceFunc[N]) Combine(p Partial[N]) {
+	if m.val == m.max || p.Value < m.val {
+		m.val = p.Value
+	}
+}
+
+func (m minReduceFunc[N]) Val() N {
+	return m.val
+}
+
+func (m *minReduceFunc[N]) Reset() {
+	m.val = m.max
+}
diff --git a/pkg/query/logical/measure/measure_analyzer.go b/pkg/query/logical/measure/measure_analyzer.go
index f92c5127..11bdaa96 100644
--- a/pkg/query/logical/measure/measure_analyzer.go
+++ b/pkg/query/logical/measure/measure_analyzer.go
@@ -24,7 +24,6 @@ import (
 	commonv1 "github.com/apache/skywalking-banyandb/api/proto/banyandb/common/v1"
 	databasev1 "github.com/apache/skywalking-banyandb/api/proto/banyandb/database/v1"
 	measurev1 "github.com/apache/skywalking-banyandb/api/proto/banyandb/measure/v1"
-	modelv1 "github.com/apache/skywalking-banyandb/api/proto/banyandb/model/v1"
 	"github.com/apache/skywalking-banyandb/pkg/query/executor"
 	"github.com/apache/skywalking-banyandb/pkg/query/logical"
 )
@@ -123,6 +122,8 @@ func Analyze(criteria *measurev1.QueryRequest, metadata []*commonv1.Metadata, ss
 			logical.NewField(criteria.GetAgg().GetFieldName()),
 			criteria.GetAgg().GetFunction(),
 			criteria.GetGroupBy() != nil,
+			criteria.GetAggReturnPartial(),
+			false,
 		)
 		pushedLimit = math.MaxInt
 	}
@@ -157,16 +158,8 @@ func DistributedAnalyze(criteria *measurev1.QueryRequest, ss []logical.Schema) (
 		}
 	}
 
-	// TODO: to support all aggregation functions
-	needCompletePushDownAgg := criteria.GetAgg() != nil &&
-		(criteria.GetAgg().GetFunction() == modelv1.AggregationFunction_AGGREGATION_FUNCTION_MAX ||
-			criteria.GetAgg().GetFunction() == modelv1.AggregationFunction_AGGREGATION_FUNCTION_MIN ||
-			criteria.GetAgg().GetFunction() == modelv1.AggregationFunction_AGGREGATION_FUNCTION_SUM ||
-			criteria.GetAgg().GetFunction() == modelv1.AggregationFunction_AGGREGATION_FUNCTION_COUNT) &&
-		criteria.GetTop() == nil
-
-	// parse fields
-	plan := newUnresolvedDistributed(criteria, needCompletePushDownAgg)
+	pushDownAgg := criteria.GetAgg() != nil && criteria.GetTop() == nil
+	plan := newUnresolvedDistributed(criteria, pushDownAgg)
 
 	// parse limit and offset
 	limitParameter := criteria.GetLimit()
@@ -181,14 +174,12 @@ func DistributedAnalyze(criteria *measurev1.QueryRequest, ss []logical.Schema) (
 	}
 
 	if criteria.GetAgg() != nil {
-		aggrFunc := criteria.GetAgg().GetFunction()
-		if needCompletePushDownAgg && aggrFunc == modelv1.AggregationFunction_AGGREGATION_FUNCTION_COUNT {
-			aggrFunc = modelv1.AggregationFunction_AGGREGATION_FUNCTION_SUM
-		}
 		plan = newUnresolvedAggregation(plan,
 			logical.NewField(criteria.GetAgg().GetFieldName()),
-			aggrFunc,
+			criteria.GetAgg().GetFunction(),
 			criteria.GetGroupBy() != nil,
+			false,       // emitPartial: liaison does not emit partial
+			pushDownAgg, // useReduceMode: only reduce partials when push-down is active (no TopN)
 		)
 		pushedLimit = math.MaxInt
 	}
diff --git a/pkg/query/logical/measure/measure_plan_aggregation.go b/pkg/query/logical/measure/measure_plan_aggregation.go
index ec2b06a6..8707ff9c 100644
--- a/pkg/query/logical/measure/measure_plan_aggregation.go
+++ b/pkg/query/logical/measure/measure_plan_aggregation.go
@@ -43,14 +43,20 @@ type unresolvedAggregation struct {
 	aggregationField *logical.Field
 	aggrFunc         modelv1.AggregationFunction
 	isGroup          bool
+	emitPartial      bool
+	useReduceMode    bool
 }
 
-func newUnresolvedAggregation(input logical.UnresolvedPlan, aggrField *logical.Field, aggrFunc modelv1.AggregationFunction, isGroup bool) logical.UnresolvedPlan {
+func newUnresolvedAggregation(input logical.UnresolvedPlan, aggrField *logical.Field, aggrFunc modelv1.AggregationFunction,
+	isGroup bool, emitPartial bool, useReduceMode bool,
+) logical.UnresolvedPlan {
 	return &unresolvedAggregation{
 		unresolvedInput:  input,
 		aggrFunc:         aggrFunc,
 		aggregationField: aggrField,
 		isGroup:          isGroup,
+		emitPartial:      emitPartial,
+		useReduceMode:    useReduceMode,
 	}
 }
 
@@ -83,27 +89,47 @@ type aggregationPlan[N aggregation.Number] struct {
 	*logical.Parent
 	schema              logical.Schema
 	aggregationFieldRef *logical.FieldRef
-	aggrFunc            aggregation.Func[N]
+	mapFunc             aggregation.Map[N]
+	reduceFunc          aggregation.Reduce[N]
+	groupByTagsRefs     [][]*logical.TagRef
 	aggrType            modelv1.AggregationFunction
 	isGroup             bool
+	emitPartial         bool
+	useReduceMode       bool
 }
 
 func newAggregationPlan[N aggregation.Number](gba *unresolvedAggregation, prevPlan logical.Plan,
 	measureSchema logical.Schema, fieldRef *logical.FieldRef,
 ) (*aggregationPlan[N], error) {
-	aggrFunc, err := aggregation.NewFunc[N](gba.aggrFunc)
+	mapFunc, err := aggregation.NewMap[N](gba.aggrFunc)
 	if err != nil {
 		return nil, err
 	}
+	var reduceFunc aggregation.Reduce[N]
+	var groupByTagsRefs [][]*logical.TagRef
+	if gba.useReduceMode {
+		reduceFunc, err = aggregation.NewReduce[N](gba.aggrFunc)
+		if err != nil {
+			return nil, err
+		}
+		if gp, ok := prevPlan.(*groupBy); ok {
+			groupByTagsRefs = gp.groupByTagsRefs
+		}
+	}
 	return &aggregationPlan[N]{
 		Parent: &logical.Parent{
 			UnresolvedInput: gba.unresolvedInput,
 			Input:           prevPlan,
 		},
 		schema:              measureSchema,
-		aggrFunc:            aggrFunc,
+		mapFunc:             mapFunc,
+		reduceFunc:          reduceFunc,
 		aggregationFieldRef: fieldRef,
+		aggrType:            gba.aggrFunc,
 		isGroup:             gba.isGroup,
+		emitPartial:         gba.emitPartial,
+		useReduceMode:       gba.useReduceMode,
+		groupByTagsRefs:     groupByTagsRefs,
 	}, nil
 }
 
@@ -127,29 +153,37 @@ func (g *aggregationPlan[N]) Execute(ec context.Context) (executor.MIterator, er
 	if err != nil {
 		return nil, err
 	}
+	if g.useReduceMode {
+		return newReduceGroupIterator(iter, g.aggregationFieldRef, g.reduceFunc, g.aggrType, g.groupByTagsRefs), nil
+	}
 	if g.isGroup {
-		return newAggGroupMIterator(iter, g.aggregationFieldRef, g.aggrFunc), nil
+		return newAggGroupMIterator(iter, g.aggregationFieldRef, g.mapFunc, g.aggrType, g.emitPartial), nil
 	}
-	return newAggAllIterator(iter, g.aggregationFieldRef, g.aggrFunc), nil
+	return newAggAllIterator(iter, g.aggregationFieldRef, g.mapFunc, g.aggrType, g.emitPartial), nil
 }
 
 type aggGroupIterator[N aggregation.Number] struct {
 	prev                executor.MIterator
 	aggregationFieldRef *logical.FieldRef
-	aggrFunc            aggregation.Func[N]
-
-	err error
+	mapFunc             aggregation.Map[N]
+	err                 error
+	aggrType            modelv1.AggregationFunction
+	emitPartial         bool
 }
 
 func newAggGroupMIterator[N aggregation.Number](
 	prev executor.MIterator,
 	aggregationFieldRef *logical.FieldRef,
-	aggrFunc aggregation.Func[N],
+	mapFunc aggregation.Map[N],
+	aggrType modelv1.AggregationFunction,
+	emitPartial bool,
 ) executor.MIterator {
 	return &aggGroupIterator[N]{
 		prev:                prev,
 		aggregationFieldRef: aggregationFieldRef,
-		aggrFunc:            aggrFunc,
+		mapFunc:             mapFunc,
+		aggrType:            aggrType,
+		emitPartial:         emitPartial,
 	}
 }
 
@@ -164,7 +198,7 @@ func (ami *aggGroupIterator[N]) Current() []*measurev1.InternalDataPoint {
 	if ami.err != nil {
 		return nil
 	}
-	ami.aggrFunc.Reset()
+	ami.mapFunc.Reset()
 	group := ami.prev.Current()
 	var resultDp *measurev1.DataPoint
 	var shardID uint32
@@ -177,7 +211,7 @@ func (ami *aggGroupIterator[N]) Current() []*measurev1.InternalDataPoint {
 			ami.err = err
 			return nil
 		}
-		ami.aggrFunc.In(v)
+		ami.mapFunc.In(v)
 		if resultDp != nil {
 			continue
 		}
@@ -189,16 +223,30 @@ func (ami *aggGroupIterator[N]) Current() []*measurev1.InternalDataPoint {
 	if resultDp == nil {
 		return nil
 	}
-	val, err := aggregation.ToFieldValue(ami.aggrFunc.Val())
-	if err != nil {
-		ami.err = err
-		return nil
-	}
-	resultDp.Fields = []*measurev1.DataPoint_Field{
-		{
-			Name:  ami.aggregationFieldRef.Field.Name,
-			Value: val,
-		},
+	if ami.emitPartial {
+		part := ami.mapFunc.Partial()
+		fvs, err := aggregation.PartialToFieldValues(ami.aggrType, part)
+		if err != nil {
+			ami.err = err
+			return nil
+		}
+		resultDp.Fields = make([]*measurev1.DataPoint_Field, len(fvs))
+		for i, fv := range fvs {
+			name := ami.aggregationFieldRef.Field.Name
+			if i > 0 {
+				name = "__agg_count"
+			}
+			resultDp.Fields[i] = &measurev1.DataPoint_Field{Name: name, Value: fv}
+		}
+	} else {
+		val, err := aggregation.ToFieldValue(ami.mapFunc.Val())
+		if err != nil {
+			ami.err = err
+			return nil
+		}
+		resultDp.Fields = []*measurev1.DataPoint_Field{
+			{Name: ami.aggregationFieldRef.Field.Name, Value: val},
+		}
 	}
 	return []*measurev1.InternalDataPoint{{DataPoint: resultDp, ShardId: shardID}}
 }
@@ -210,21 +258,26 @@ func (ami *aggGroupIterator[N]) Close() error {
 type aggAllIterator[N aggregation.Number] struct {
 	prev                executor.MIterator
 	aggregationFieldRef *logical.FieldRef
-	aggrFunc            aggregation.Func[N]
-
-	result *measurev1.DataPoint
-	err    error
+	mapFunc             aggregation.Map[N]
+	result              *measurev1.DataPoint
+	err                 error
+	aggrType            modelv1.AggregationFunction
+	emitPartial         bool
 }
 
 func newAggAllIterator[N aggregation.Number](
 	prev executor.MIterator,
 	aggregationFieldRef *logical.FieldRef,
-	aggrFunc aggregation.Func[N],
+	mapFunc aggregation.Map[N],
+	aggrType modelv1.AggregationFunction,
+	emitPartial bool,
 ) executor.MIterator {
 	return &aggAllIterator[N]{
 		prev:                prev,
 		aggregationFieldRef: aggregationFieldRef,
-		aggrFunc:            aggrFunc,
+		mapFunc:             mapFunc,
+		aggrType:            aggrType,
+		emitPartial:         emitPartial,
 	}
 }
 
@@ -244,7 +297,7 @@ func (ami *aggAllIterator[N]) Next() bool {
 				ami.err = err
 				return false
 			}
-			ami.aggrFunc.In(v)
+			ami.mapFunc.In(v)
 			if resultDp != nil {
 				continue
 			}
@@ -256,16 +309,30 @@ func (ami *aggAllIterator[N]) Next() bool {
 	if resultDp == nil {
 		return false
 	}
-	val, err := aggregation.ToFieldValue(ami.aggrFunc.Val())
-	if err != nil {
-		ami.err = err
-		return false
-	}
-	resultDp.Fields = []*measurev1.DataPoint_Field{
-		{
-			Name:  ami.aggregationFieldRef.Field.Name,
-			Value: val,
-		},
+	if ami.emitPartial {
+		part := ami.mapFunc.Partial()
+		fvs, err := aggregation.PartialToFieldValues(ami.aggrType, part)
+		if err != nil {
+			ami.err = err
+			return false
+		}
+		resultDp.Fields = make([]*measurev1.DataPoint_Field, len(fvs))
+		for i, fv := range fvs {
+			name := ami.aggregationFieldRef.Field.Name
+			if i > 0 {
+				name = "__agg_count"
+			}
+			resultDp.Fields[i] = &measurev1.DataPoint_Field{Name: name, Value: fv}
+		}
+	} else {
+		val, err := aggregation.ToFieldValue(ami.mapFunc.Val())
+		if err != nil {
+			ami.err = err
+			return false
+		}
+		resultDp.Fields = []*measurev1.DataPoint_Field{
+			{Name: ami.aggregationFieldRef.Field.Name, Value: val},
+		}
 	}
 	ami.result = resultDp
 	return true
@@ -282,3 +349,114 @@ func (ami *aggAllIterator[N]) Current() []*measurev1.InternalDataPoint {
 func (ami *aggAllIterator[N]) Close() error {
 	return ami.prev.Close()
 }
+
+type reduceGroupIterator[N aggregation.Number] struct {
+	prev                executor.MIterator
+	aggregationFieldRef *logical.FieldRef
+	reduceFunc          aggregation.Reduce[N]
+	err                 error
+	groupMap            map[uint64][]*measurev1.InternalDataPoint
+	groupByTagsRefs     [][]*logical.TagRef
+	groupKeys           []uint64
+	aggrType            modelv1.AggregationFunction
+	index               int
+}
+
+func newReduceGroupIterator[N aggregation.Number](
+	prev executor.MIterator,
+	aggregationFieldRef *logical.FieldRef,
+	reduceFunc aggregation.Reduce[N],
+	aggrType modelv1.AggregationFunction,
+	groupByTagsRefs [][]*logical.TagRef,
+) executor.MIterator {
+	return &reduceGroupIterator[N]{
+		prev:                prev,
+		aggregationFieldRef: aggregationFieldRef,
+		reduceFunc:          reduceFunc,
+		aggrType:            aggrType,
+		groupByTagsRefs:     groupByTagsRefs,
+		groupMap:            make(map[uint64][]*measurev1.InternalDataPoint),
+	}
+}
+
+func (rgi *reduceGroupIterator[N]) loadGroups() bool {
+	if rgi.groupKeys != nil {
+		return true
+	}
+	for rgi.prev.Next() {
+		group := rgi.prev.Current()
+		for _, idp := range group {
+			key, keyErr := formatGroupByKey(idp.GetDataPoint(), rgi.groupByTagsRefs)
+			if keyErr != nil {
+				rgi.err = keyErr
+				return false
+			}
+			rgi.groupMap[key] = append(rgi.groupMap[key], idp)
+		}
+	}
+	if closeErr := rgi.prev.Close(); closeErr != nil {
+		rgi.err = closeErr
+		return false
+	}
+	rgi.groupKeys = make([]uint64, 0, len(rgi.groupMap))
+	for k := range rgi.groupMap {
+		rgi.groupKeys = append(rgi.groupKeys, k)
+	}
+	return true
+}
+
+func (rgi *reduceGroupIterator[N]) Next() bool {
+	if rgi.err != nil {
+		return false
+	}
+	if !rgi.loadGroups() {
+		return false
+	}
+	return rgi.index < len(rgi.groupKeys)
+}
+
+func (rgi *reduceGroupIterator[N]) Current() []*measurev1.InternalDataPoint {
+	if rgi.err != nil || rgi.groupKeys == nil || rgi.index >= len(rgi.groupKeys) {
+		return nil
+	}
+	key := rgi.groupKeys[rgi.index]
+	rgi.index++
+	idps := rgi.groupMap[key]
+	if len(idps) == 0 {
+		return nil
+	}
+	rgi.reduceFunc.Reset()
+	var resultDp *measurev1.DataPoint
+	for _, idp := range idps {
+		dp := idp.GetDataPoint()
+		fvs := make([]*modelv1.FieldValue, len(dp.GetFields()))
+		for i, f := range dp.GetFields() {
+			fvs[i] = f.GetValue()
+		}
+		part, partErr := aggregation.FieldValuesToPartial[N](rgi.aggrType, fvs)
+		if partErr != nil {
+			rgi.err = partErr
+			return nil
+		}
+		rgi.reduceFunc.Combine(part)
+		if resultDp == nil {
+			resultDp = &measurev1.DataPoint{TagFamilies: dp.TagFamilies}
+		}
+	}
+	if resultDp == nil {
+		return nil
+	}
+	val, err := aggregation.ToFieldValue(rgi.reduceFunc.Val())
+	if err != nil {
+		rgi.err = err
+		return nil
+	}
+	resultDp.Fields = []*measurev1.DataPoint_Field{
+		{Name: rgi.aggregationFieldRef.Field.Name, Value: val},
+	}
+	return []*measurev1.InternalDataPoint{{DataPoint: resultDp, ShardId: 0}}
+}
+
+func (rgi *reduceGroupIterator[N]) Close() error {
+	return rgi.prev.Close()
+}
diff --git a/pkg/query/logical/measure/measure_plan_distributed.go b/pkg/query/logical/measure/measure_plan_distributed.go
index d81d3f68..55f35967 100644
--- a/pkg/query/logical/measure/measure_plan_distributed.go
+++ b/pkg/query/logical/measure/measure_plan_distributed.go
@@ -103,15 +103,15 @@ func (as *pushDownAggSchema) Children() []logical.Schema {
 }
 
 type unresolvedDistributed struct {
-	originalQuery           *measurev1.QueryRequest
-	groupByEntity           bool
-	needCompletePushDownAgg bool
+	originalQuery *measurev1.QueryRequest
+	groupByEntity bool
+	pushDownAgg   bool
 }
 
-func newUnresolvedDistributed(query *measurev1.QueryRequest, needCompletePushDownAgg bool) logical.UnresolvedPlan {
+func newUnresolvedDistributed(query *measurev1.QueryRequest, pushDownAgg bool) logical.UnresolvedPlan {
 	return &unresolvedDistributed{
-		originalQuery:           query,
-		needCompletePushDownAgg: needCompletePushDownAgg,
+		originalQuery: query,
+		pushDownAgg:   pushDownAgg,
 	}
 }
 
@@ -150,9 +150,10 @@ func (ud *unresolvedDistributed) Analyze(s logical.Schema) (logical.Plan, error)
 		Limit:           limit + ud.originalQuery.Offset,
 		OrderBy:         ud.originalQuery.OrderBy,
 	}
-	if ud.needCompletePushDownAgg {
+	if ud.pushDownAgg {
 		temp.GroupBy = ud.originalQuery.GroupBy
 		temp.Agg = ud.originalQuery.Agg
+		temp.AggReturnPartial = true
 	}
 	// push down groupBy, agg and top to data node and rewrite agg result to raw data
 	if ud.originalQuery.Agg != nil && ud.originalQuery.Top != nil {
@@ -163,7 +164,7 @@ func (ud *unresolvedDistributed) Analyze(s logical.Schema) (logical.Plan, error)
 	}
 	// Prepare groupBy tags refs if needed for deduplication
 	var groupByTagsRefs [][]*logical.TagRef
-	if ud.needCompletePushDownAgg && ud.originalQuery.GetGroupBy() != nil {
+	if ud.pushDownAgg && ud.originalQuery.GetGroupBy() != nil {
 		groupByTags := logical.ToTags(ud.originalQuery.GetGroupBy().GetTagProjection())
 		var err error
 		groupByTagsRefs, err = s.CreateTagRef(groupByTags...)
@@ -179,12 +180,12 @@ func (ud *unresolvedDistributed) Analyze(s logical.Schema) (logical.Plan, error)
 			return nil, fmt.Errorf("entity tag %s not found", e)
 		}
 		result := &distributedPlan{
-			queryTemplate:           temp,
-			s:                       s,
-			sortByTime:              false,
-			sortTagSpec:             *sortTagSpec,
-			needCompletePushDownAgg: ud.needCompletePushDownAgg,
-			groupByTagsRefs:         groupByTagsRefs,
+			queryTemplate:   temp,
+			s:               s,
+			sortByTime:      false,
+			sortTagSpec:     *sortTagSpec,
+			pushDownAgg:     ud.pushDownAgg,
+			groupByTagsRefs: groupByTagsRefs,
 		}
 		if ud.originalQuery.OrderBy != nil && ud.originalQuery.OrderBy.Sort == modelv1.Sort_SORT_DESC {
 			result.desc = true
@@ -193,20 +194,20 @@ func (ud *unresolvedDistributed) Analyze(s logical.Schema) (logical.Plan, error)
 	}
 	if ud.originalQuery.OrderBy == nil {
 		return &distributedPlan{
-			queryTemplate:           temp,
-			s:                       s,
-			sortByTime:              true,
-			needCompletePushDownAgg: ud.needCompletePushDownAgg,
-			groupByTagsRefs:         groupByTagsRefs,
+			queryTemplate:   temp,
+			s:               s,
+			sortByTime:      true,
+			pushDownAgg:     ud.pushDownAgg,
+			groupByTagsRefs: groupByTagsRefs,
 		}, nil
 	}
 	if ud.originalQuery.OrderBy.IndexRuleName == "" {
 		result := &distributedPlan{
-			queryTemplate:           temp,
-			s:                       s,
-			sortByTime:              true,
-			needCompletePushDownAgg: ud.needCompletePushDownAgg,
-			groupByTagsRefs:         groupByTagsRefs,
+			queryTemplate:   temp,
+			s:               s,
+			sortByTime:      true,
+			pushDownAgg:     ud.pushDownAgg,
+			groupByTagsRefs: groupByTagsRefs,
 		}
 		if ud.originalQuery.OrderBy.Sort == modelv1.Sort_SORT_DESC {
 			result.desc = true
@@ -225,12 +226,12 @@ func (ud *unresolvedDistributed) Analyze(s logical.Schema) (logical.Plan, error)
 		return nil, fmt.Errorf("tag %s not found", indexRule.Tags[0])
 	}
 	result := &distributedPlan{
-		queryTemplate:           temp,
-		s:                       s,
-		sortByTime:              false,
-		sortTagSpec:             *sortTagSpec,
-		needCompletePushDownAgg: ud.needCompletePushDownAgg,
-		groupByTagsRefs:         groupByTagsRefs,
+		queryTemplate:   temp,
+		s:               s,
+		sortByTime:      false,
+		sortTagSpec:     *sortTagSpec,
+		pushDownAgg:     ud.pushDownAgg,
+		groupByTagsRefs: groupByTagsRefs,
 	}
 	if ud.originalQuery.OrderBy.Sort == modelv1.Sort_SORT_DESC {
 		result.desc = true
@@ -239,14 +240,14 @@ func (ud *unresolvedDistributed) Analyze(s logical.Schema) (logical.Plan, error)
 }
 
 type distributedPlan struct {
-	s                       logical.Schema
-	queryTemplate           *measurev1.QueryRequest
-	sortTagSpec             logical.TagSpec
-	groupByTagsRefs         [][]*logical.TagRef
-	maxDataPointsSize       uint32
-	sortByTime              bool
-	desc                    bool
-	needCompletePushDownAgg bool
+	s                 logical.Schema
+	queryTemplate     *measurev1.QueryRequest
+	sortTagSpec       logical.TagSpec
+	groupByTagsRefs   [][]*logical.TagRef
+	maxDataPointsSize uint32
+	sortByTime        bool
+	desc              bool
+	pushDownAgg       bool
 }
 
 func (t *distributedPlan) Execute(ctx context.Context) (mi executor.MIterator, err error) {
@@ -292,7 +293,7 @@ func (t *distributedPlan) Execute(ctx context.Context) (mi executor.MIterator, e
 				if span != nil {
 					span.AddSubTrace(d.Trace)
 				}
-				if t.needCompletePushDownAgg {
+				if t.pushDownAgg {
 					pushedDownAggDps = append(pushedDownAggDps, d.DataPoints...)
 					dataPointCount += len(d.DataPoints)
 					continue
@@ -310,7 +311,7 @@ func (t *distributedPlan) Execute(ctx context.Context) (mi executor.MIterator, e
 		span.Tagf("response_count", "%d", responseCount)
 		span.Tagf("data_point_count", "%d", dataPointCount)
 	}
-	if t.needCompletePushDownAgg {
+	if t.pushDownAgg {
 		deduplicatedDps, dedupErr := deduplicateAggregatedDataPointsWithShard(pushedDownAggDps, t.groupByTagsRefs)
 		if dedupErr != nil {
 			return nil, multierr.Append(err, dedupErr)
@@ -333,7 +334,7 @@ func (t *distributedPlan) Children() []logical.Plan {
 }
 
 func (t *distributedPlan) Schema() logical.Schema {
-	if t.needCompletePushDownAgg {
+	if t.pushDownAgg {
 		return &pushDownAggSchema{
 			originalSchema:   t.s,
 			aggregationField: logical.NewField(t.queryTemplate.Agg.FieldName),
diff --git a/pkg/query/logical/measure/topn_analyzer.go b/pkg/query/logical/measure/topn_analyzer.go
index 34aced93..cf2d87b6 100644
--- a/pkg/query/logical/measure/topn_analyzer.go
+++ b/pkg/query/logical/measure/topn_analyzer.go
@@ -116,7 +116,9 @@ func TopNAnalyze(criteria *measurev1.TopNRequest, sourceMeasureSchemaList []*dat
 		plan = newUnresolvedAggregation(plan,
 			&logical.Field{Name: topNAggSchema.FieldName},
 			criteria.GetAgg(),
-			true)
+			true,
+			false,
+			false)
 	}
 
 	plan = top(plan, &measurev1.QueryRequest_Top{
diff --git a/test/cases/measure/data/input/group_mean.ql b/test/cases/measure/data/input/group_mean.ql
new file mode 100644
index 00000000..61e648a3
--- /dev/null
+++ b/test/cases/measure/data/input/group_mean.ql
@@ -0,0 +1,21 @@
+# Licensed to Apache Software Foundation (ASF) under one or more contributor
+# license agreements. See the NOTICE file distributed with
+# this work for additional information regarding copyright
+# ownership. Apache Software Foundation (ASF) licenses this file to you under
+# the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+
+SELECT id, total::field, value::field, MEAN(value) FROM MEASURE service_cpm_minute IN sw_metric
+TIME > '-15m'
+GROUP BY id, value
\ No newline at end of file
diff --git a/test/cases/measure/data/input/group_mean.yaml b/test/cases/measure/data/input/group_mean.yaml
new file mode 100644
index 00000000..b35af2e2
--- /dev/null
+++ b/test/cases/measure/data/input/group_mean.yaml
@@ -0,0 +1,34 @@
+# Licensed to Apache Software Foundation (ASF) under one or more contributor
+# license agreements. See the NOTICE file distributed with
+# this work for additional information regarding copyright
+# ownership. Apache Software Foundation (ASF) licenses this file to you under
+# the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+name: "service_cpm_minute"
+groups: ["sw_metric"]
+tagProjection:
+  tagFamilies:
+  - name: "default"
+    tags: ["id"]
+fieldProjection:
+  names: ["total", "value"]
+groupBy:
+  tagProjection:
+    tagFamilies:
+    - name: "default"
+      tags: ["id"]
+  fieldName: "value"
+agg:
+  function: "AGGREGATION_FUNCTION_MEAN"
+  fieldName: "value"
\ No newline at end of file
diff --git a/test/cases/measure/data/want/group_mean.yaml b/test/cases/measure/data/want/group_mean.yaml
new file mode 100644
index 00000000..82d0e8cd
--- /dev/null
+++ b/test/cases/measure/data/want/group_mean.yaml
@@ -0,0 +1,54 @@
+# Licensed to Apache Software Foundation (ASF) under one or more contributor
+# license agreements. See the NOTICE file distributed with
+# this work for additional information regarding copyright
+# ownership. Apache Software Foundation (ASF) licenses this file to you under
+# the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+dataPoints:
+- fields:
+  - name: value
+    value:
+      int:
+        value: "2"
+  tagFamilies:
+  - name: default
+    tags:
+    - key: id
+      value:
+        str:
+          value: svc1
+- fields:
+  - name: value
+    value:
+      int:
+        value: "4"
+  tagFamilies:
+  - name: default
+    tags:
+    - key: id
+      value:
+        str:
+          value: svc2
+- fields:
+  - name: value
+    value:
+      int:
+        value: "6"
+  tagFamilies:
+  - name: default
+    tags:
+    - key: id
+      value:
+        str:
+          value: svc3
diff --git a/test/cases/measure/measure.go b/test/cases/measure/measure.go
index 2679c8b3..fadf4ee0 100644
--- a/test/cases/measure/measure.go
+++ b/test/cases/measure/measure.go
@@ -52,6 +52,7 @@ var _ = g.DescribeTable("Scanning Measures", verify,
 	g.Entry("group and min", helpers.Args{Input: "group_min", Duration: 25 * time.Minute, Offset: -20 * time.Minute}),
 	g.Entry("group and sum", helpers.Args{Input: "group_sum", Duration: 25 * time.Minute, Offset: -20 * time.Minute}),
 	g.Entry("group and count", helpers.Args{Input: "group_count", Duration: 25 * time.Minute, Offset: -20 * time.Minute}),
+	g.Entry("group and mean", helpers.Args{Input: "group_mean", Duration: 25 * time.Minute, Offset: -20 * time.Minute}),
 	g.Entry("group without field", helpers.Args{Input: "group_no_field", Duration: 25 * time.Minute, Offset: -20 * time.Minute}),
 	g.Entry("top 2 by id", helpers.Args{Input: "top", Duration: 25 * time.Minute, Offset: -20 * time.Minute}),
 	g.Entry("bottom 2 by id", helpers.Args{Input: "bottom", Duration: 25 * time.Minute, Offset: -20 * time.Minute}),

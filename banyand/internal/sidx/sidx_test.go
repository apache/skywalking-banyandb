// Licensed to Apache Software Foundation (ASF) under one or more contributor
// license agreements. See the NOTICE file distributed with
// this work for additional information regarding copyright
// ownership. Apache Software Foundation (ASF) licenses this file to you under
// the Apache License, Version 2.0 (the "License"); you may
// not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.  See the License for the
// specific language governing permissions and limitations
// under the License.

package sidx

import (
	"context"
	"fmt"
	"sort"
	"sync"
	"testing"
	"time"

	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"

	"github.com/apache/skywalking-banyandb/api/common"
	modelv1 "github.com/apache/skywalking-banyandb/api/proto/banyandb/model/v1"
	"github.com/apache/skywalking-banyandb/banyand/observability"
	"github.com/apache/skywalking-banyandb/banyand/protector"
	"github.com/apache/skywalking-banyandb/pkg/index"
	pbv1 "github.com/apache/skywalking-banyandb/pkg/pb/v1"
)

// Test helper functions.

func waitForIntroducerLoop() {
	// Small delay to allow the introducer loop to process writes
	time.Sleep(10 * time.Millisecond)
}

func createTestOptions(t *testing.T) *Options {
	opts := NewDefaultOptions()
	opts.Memory = protector.NewMemory(observability.NewBypassRegistry())
	opts.Path = t.TempDir() // Use temporary directory for tests
	return opts
}

func createTestSIDX(t *testing.T) SIDX {
	opts := createTestOptions(t)
	sidx, err := NewSIDX(opts)
	require.NoError(t, err)
	require.NotNil(t, sidx)
	return sidx
}

func createTestTag(name, value string) Tag {
	return Tag{
		Name:      name,
		Value:     []byte(value),
		ValueType: pbv1.ValueTypeStr,
	}
}

func createTestWriteRequest(seriesID common.SeriesID, key int64, data string, tags ...Tag) WriteRequest {
	return WriteRequest{
		SeriesID: seriesID,
		Key:      key,
		Data:     []byte(data),
		Tags:     tags,
	}
}

func createTestQueryRequest(seriesIDs ...common.SeriesID) QueryRequest {
	return QueryRequest{
		SeriesIDs: seriesIDs,
	}
}

// Write Operation Tests.

func TestSIDX_Write_SingleRequest(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()

	// Test single write request
	reqs := []WriteRequest{
		createTestWriteRequest(1, 100, "data1", createTestTag("tag1", "value1")),
	}

	err := sidx.Write(ctx, reqs)
	assert.NoError(t, err)

	// Verify stats
	stats, err := sidx.Stats(ctx)
	require.NoError(t, err)
	assert.Equal(t, int64(1), stats.WriteCount.Load())
}

func TestSIDX_Write_BatchRequest(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()

	// Test batch write requests
	reqs := []WriteRequest{
		createTestWriteRequest(1, 100, "data1", createTestTag("tag1", "value1")),
		createTestWriteRequest(1, 101, "data2", createTestTag("tag1", "value2")),
		createTestWriteRequest(2, 200, "data3", createTestTag("tag2", "value3")),
	}

	err := sidx.Write(ctx, reqs)
	assert.NoError(t, err)

	// Verify stats
	stats, err := sidx.Stats(ctx)
	require.NoError(t, err)
	assert.Equal(t, int64(1), stats.WriteCount.Load()) // One batch write
}

func TestSIDX_Write_Validation(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()

	tests := []struct {
		name      string
		req       WriteRequest
		expectErr bool
	}{
		{
			name:      "valid request",
			req:       createTestWriteRequest(1, 100, "data1"),
			expectErr: false,
		},
		{
			name:      "zero series ID",
			req:       WriteRequest{SeriesID: 0, Key: 100, Data: []byte("data")},
			expectErr: true,
		},
		{
			name:      "nil data",
			req:       WriteRequest{SeriesID: 1, Key: 100, Data: nil},
			expectErr: true,
		},
		{
			name:      "empty data",
			req:       WriteRequest{SeriesID: 1, Key: 100, Data: []byte("")},
			expectErr: true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			err := sidx.Write(ctx, []WriteRequest{tt.req})
			if tt.expectErr {
				assert.Error(t, err)
			} else {
				assert.NoError(t, err)
			}
		})
	}
}

func TestSIDX_Write_WithTags(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()

	// Write with multiple tags
	tags := []Tag{
		createTestTag("service", "user-service"),
		createTestTag("endpoint", "/api/users"),
		createTestTag("status", "200"),
	}

	reqs := []WriteRequest{
		createTestWriteRequest(1, 100, "trace-data", tags...),
	}

	err := sidx.Write(ctx, reqs)
	assert.NoError(t, err)
}

// Query Operation Tests.

func TestSIDX_Query_BasicQuery(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()

	// Write test data
	reqs := []WriteRequest{
		createTestWriteRequest(1, 100, "data1"),
		createTestWriteRequest(1, 101, "data2"),
	}
	err := sidx.Write(ctx, reqs)
	require.NoError(t, err)

	// Wait for introducer loop to process
	waitForIntroducerLoop()

	// Query the data
	queryReq := createTestQueryRequest(1)
	response, err := sidx.Query(ctx, queryReq)
	require.NoError(t, err)
	require.NotNil(t, response)

	// Validate results
	if response.Len() > 0 {
		assert.Greater(t, response.Len(), 0)
	}
}

func TestSIDX_Query_EmptyResult(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()

	// Query without writing any data
	queryReq := createTestQueryRequest(999) // Non-existent series
	response, err := sidx.Query(ctx, queryReq)
	require.NoError(t, err)
	require.NotNil(t, response)

	// Should return empty result
	assert.Equal(t, 0, response.Len()) // Empty result returns length 0
}

func TestSIDX_Query_KeyRangeFilter(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()

	// Write test data with different keys
	reqs := []WriteRequest{
		createTestWriteRequest(1, 100, "data100"),
		createTestWriteRequest(1, 150, "data150"),
		createTestWriteRequest(1, 200, "data200"),
	}
	err := sidx.Write(ctx, reqs)
	require.NoError(t, err)

	// Wait for introducer loop to process
	waitForIntroducerLoop()

	// Query with key range
	minKey := int64(120)
	maxKey := int64(180)
	queryReq := QueryRequest{
		SeriesIDs: []common.SeriesID{1},
		MinKey:    &minKey,
		MaxKey:    &maxKey,
	}

	response, err := sidx.Query(ctx, queryReq)
	require.NoError(t, err)
	require.NotNil(t, response)

	// Should only return data150
	if response.Len() > 0 {
		// Verify keys are within range
		for _, key := range response.Keys {
			assert.GreaterOrEqual(t, key, minKey)
			assert.LessOrEqual(t, key, maxKey)
		}
	}
}

func TestSIDX_Query_Ordering(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()

	// Write data from different seriesIDs in non-sorted order to expose ordering bugs
	reqs := []WriteRequest{
		// Series 1 data
		createTestWriteRequest(1, 300, "series1-data300"),
		createTestWriteRequest(1, 100, "series1-data100"),
		createTestWriteRequest(1, 200, "series1-data200"),

		// Series 2 data
		createTestWriteRequest(2, 250, "series2-data250"),
		createTestWriteRequest(2, 150, "series2-data150"),
		createTestWriteRequest(2, 50, "series2-data50"),

		// Series 3 data
		createTestWriteRequest(3, 350, "series3-data350"),
		createTestWriteRequest(3, 75, "series3-data75"),
		createTestWriteRequest(3, 175, "series3-data175"),
	}
	err := sidx.Write(ctx, reqs)
	require.NoError(t, err)

	// Wait for introducer loop to process
	waitForIntroducerLoop()

	tests := []struct {
		order     *index.OrderBy
		name      string
		seriesIDs []common.SeriesID
		ascending bool
	}{
		{
			name:      "ascending order single series",
			ascending: true,
			order:     &index.OrderBy{Sort: modelv1.Sort_SORT_ASC},
			seriesIDs: []common.SeriesID{1},
		},
		{
			name:      "descending order single series",
			ascending: false,
			order:     &index.OrderBy{Sort: modelv1.Sort_SORT_DESC},
			seriesIDs: []common.SeriesID{1},
		},
		{
			name:      "ascending order multiple series",
			ascending: true,
			order:     &index.OrderBy{Sort: modelv1.Sort_SORT_ASC},
			seriesIDs: []common.SeriesID{1, 2, 3},
		},
		{
			name:      "descending order multiple series",
			ascending: false,
			order:     &index.OrderBy{Sort: modelv1.Sort_SORT_DESC},
			seriesIDs: []common.SeriesID{1, 2, 3},
		},
		{
			name:      "default order multiple series",
			ascending: true,
			order:     nil, // Should default to ascending
			seriesIDs: []common.SeriesID{1, 2, 3},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			queryReq := QueryRequest{
				SeriesIDs: tt.seriesIDs,
				Order:     tt.order,
			}

			response, err := sidx.Query(ctx, queryReq)
			require.NoError(t, err)
			require.NotNil(t, response)

			// Get all keys directly from response
			allKeys := response.Keys

			// Debug: Print the keys and data to understand the ordering
			t.Logf("Query with series %v, order %v", tt.seriesIDs, tt.order)
			for i, key := range allKeys {
				if i < len(response.Data) && i < len(response.SIDs) {
					t.Logf("  Key: %d, Data: %s, SeriesID: %d", key, string(response.Data[i]), response.SIDs[i])
				}
			}

			// Verify ordering
			if len(allKeys) > 1 {
				isSorted := sort.SliceIsSorted(allKeys, func(i, j int) bool {
					if tt.ascending {
						return allKeys[i] < allKeys[j]
					}
					return allKeys[i] > allKeys[j]
				})
				assert.True(t, isSorted, "Keys should be sorted in %s order. Keys: %v",
					map[bool]string{true: "ascending", false: "descending"}[tt.ascending], allKeys)
			}
		})
	}
}

func TestSIDX_Query_Validation(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()

	tests := []struct {
		name      string
		req       QueryRequest
		expectErr bool
	}{
		{
			name:      "valid request",
			req:       createTestQueryRequest(1),
			expectErr: false,
		},
		{
			name:      "empty series IDs",
			req:       QueryRequest{SeriesIDs: []common.SeriesID{}},
			expectErr: true,
		},
		{
			name: "invalid key range",
			req: QueryRequest{
				SeriesIDs: []common.SeriesID{1},
				MinKey:    ptrInt64(200),
				MaxKey:    ptrInt64(100), // min > max
			},
			expectErr: true,
		},
		{
			name: "negative max element size",
			req: QueryRequest{
				SeriesIDs:      []common.SeriesID{1},
				MaxElementSize: -1,
			},
			expectErr: true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			_, err := sidx.Query(ctx, tt.req)
			if tt.expectErr {
				assert.Error(t, err)
			} else {
				assert.NoError(t, err)
			}
		})
	}
}

// End-to-End Integration Tests.

func TestSIDX_WriteQueryIntegration(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()

	// Write comprehensive test dataset
	reqs := []WriteRequest{
		// Series 1 data
		createTestWriteRequest(1, 100, "series1-data1", createTestTag("env", "prod")),
		createTestWriteRequest(1, 150, "series1-data2", createTestTag("env", "prod")),
		createTestWriteRequest(1, 200, "series1-data3", createTestTag("env", "test")),

		// Series 2 data
		createTestWriteRequest(2, 120, "series2-data1", createTestTag("env", "dev")),
		createTestWriteRequest(2, 180, "series2-data2", createTestTag("env", "dev")),
	}

	err := sidx.Write(ctx, reqs)
	require.NoError(t, err)

	// Test 1: Query single series
	queryReq := createTestQueryRequest(1)
	response, err := sidx.Query(ctx, queryReq)
	require.NoError(t, err)
	require.NotNil(t, response)

	series1Keys := collectAllKeys(t, response)
	assert.Greater(t, len(series1Keys), 0, "Should find data for series 1")

	// Test 2: Query multiple series
	queryReq2 := createTestQueryRequest(1, 2)
	response2, err := sidx.Query(ctx, queryReq2)
	require.NoError(t, err)
	require.NotNil(t, response2)

	allKeys := collectAllKeys(t, response2)
	assert.GreaterOrEqual(t, len(allKeys), len(series1Keys), "Should find at least as much data for multiple series")

	// Test 3: Query with key range that spans both series
	minKey := int64(110)
	maxKey := int64(190)
	queryReq3 := QueryRequest{
		SeriesIDs: []common.SeriesID{1, 2},
		MinKey:    &minKey,
		MaxKey:    &maxKey,
	}

	response3, err := sidx.Query(ctx, queryReq3)
	require.NoError(t, err)
	require.NotNil(t, response3)

	rangeKeys := collectAllKeys(t, response3)
	for _, key := range rangeKeys {
		assert.GreaterOrEqual(t, key, minKey)
		assert.LessOrEqual(t, key, maxKey)
	}
}

func TestSIDX_DataConsistency(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()

	// Write known test data
	expectedData := map[int64]string{
		100: "data-100",
		200: "data-200",
		300: "data-300",
	}

	var reqs []WriteRequest
	for key, data := range expectedData {
		reqs = append(reqs, createTestWriteRequest(1, key, data))
	}

	err := sidx.Write(ctx, reqs)
	require.NoError(t, err)

	// Query back and verify data integrity
	queryReq := createTestQueryRequest(1)
	response, err := sidx.Query(ctx, queryReq)
	require.NoError(t, err)
	require.NotNil(t, response)

	actualData := make(map[int64]string)
	for i, key := range response.Keys {
		if i < len(response.Data) {
			actualData[key] = string(response.Data[i])
		}
	}

	// Verify all data was retrieved correctly
	for expectedKey, expectedValue := range expectedData {
		actualValue, exists := actualData[expectedKey]
		assert.True(t, exists, "Key %d should exist in results", expectedKey)
		assert.Equal(t, expectedValue, actualValue, "Data for key %d should match", expectedKey)
	}
}

// Error Handling and Edge Cases.

func TestSIDX_LargeDataset(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()

	// Write a large number of elements
	const numElements = 1000
	var reqs []WriteRequest

	for i := 0; i < numElements; i++ {
		reqs = append(reqs, createTestWriteRequest(
			common.SeriesID(i%10+1), // 10 different series
			int64(i),
			fmt.Sprintf("data-%d", i),
			createTestTag("batch", "large"),
		))
	}

	err := sidx.Write(ctx, reqs)
	require.NoError(t, err)

	// Query back and verify we can handle large result sets
	queryReq := QueryRequest{
		SeriesIDs: []common.SeriesID{1, 2, 3}, // Query subset of series
	}

	response, err := sidx.Query(ctx, queryReq)
	require.NoError(t, err)
	require.NotNil(t, response)

	totalElements := response.Len()

	assert.Greater(t, totalElements, 0, "Should find elements in large dataset")
}

// Concurrency Tests.

func TestSIDX_ConcurrentWrites(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()
	numGoroutines := 10
	elementsPerGoroutine := 100

	var wg sync.WaitGroup
	errors := make(chan error, numGoroutines)

	// Launch concurrent writers
	for g := 0; g < numGoroutines; g++ {
		wg.Add(1)
		go func(goroutineID int) {
			defer wg.Done()

			var reqs []WriteRequest
			for i := 0; i < elementsPerGoroutine; i++ {
				seriesID := common.SeriesID(goroutineID + 1)
				key := int64(goroutineID*1000 + i)
				data := fmt.Sprintf("goroutine-%d-data-%d", goroutineID, i)

				reqs = append(reqs, createTestWriteRequest(seriesID, key, data))
			}

			if err := sidx.Write(ctx, reqs); err != nil {
				errors <- err
			}
		}(g)
	}

	wg.Wait()
	close(errors)

	// Check for errors
	for err := range errors {
		assert.NoError(t, err)
	}

	// Verify stats reflect all writes
	stats, err := sidx.Stats(ctx)
	require.NoError(t, err)
	assert.Equal(t, int64(numGoroutines), stats.WriteCount.Load())
}

func TestSIDX_ConcurrentReadsWrites(t *testing.T) {
	sidx := createTestSIDX(t)
	defer func() {
		assert.NoError(t, sidx.Close())
	}()

	ctx := context.Background()

	// Pre-populate with some data
	initialReqs := []WriteRequest{
		createTestWriteRequest(1, 100, "initial-data"),
	}
	err := sidx.Write(ctx, initialReqs)
	require.NoError(t, err)

	var wg sync.WaitGroup
	numReaders := 5
	numWriters := 5
	duration := 100 * time.Millisecond

	// Launch concurrent readers
	for i := 0; i < numReaders; i++ {
		wg.Add(1)
		go func(_ int) {
			defer wg.Done()

			start := time.Now()
			for time.Since(start) < duration {
				queryReq := createTestQueryRequest(1)
				response, errQuery := sidx.Query(ctx, queryReq)
				if errQuery != nil {
					continue // Continue on error during concurrent access
				}
				if response != nil {
					_ = response.Len() // Just access the data
				}
			}
		}(i)
	}

	// Launch concurrent writers
	for i := 0; i < numWriters; i++ {
		wg.Add(1)
		go func(writerID int) {
			defer wg.Done()

			start := time.Now()
			writeCount := 0
			for time.Since(start) < duration {
				req := createTestWriteRequest(
					common.SeriesID(writerID+2),
					int64(writeCount),
					fmt.Sprintf("writer-%d-data-%d", writerID, writeCount),
				)
				sidx.Write(ctx, []WriteRequest{req}) // Ignore errors during concurrent stress
				writeCount++
			}
		}(i)
	}

	wg.Wait()

	// Final verification - should still be able to query
	queryReq := createTestQueryRequest(1)
	response, err := sidx.Query(ctx, queryReq)
	require.NoError(t, err)
	require.NotNil(t, response)
}

// Utility functions.

func ptrInt64(v int64) *int64 {
	return &v
}

func collectAllKeys(t *testing.T, response *QueryResponse) []int64 {
	require.NoError(t, nil) // No error expected since we already have the response
	return response.Keys
}

// TestQueryResult_MaxElementSize_UniqueData tests that MaxElementSize limits unique Data elements, not total elements.
func TestQueryResult_MaxElementSize_UniqueData(t *testing.T) {
	tests := []struct {
		name           string
		description    string
		inputData      [][]byte
		inputKeys      []int64
		maxElementSize int
		expectedLen    int
		expectedUnique int
	}{
		{
			name:           "no_limit",
			maxElementSize: 0, // No limit
			inputData:      [][]byte{[]byte("trace1"), []byte("trace1"), []byte("trace2"), []byte("trace2")},
			inputKeys:      []int64{100, 101, 200, 201},
			expectedLen:    4,
			expectedUnique: 2,
			description:    "No limit should include all elements",
		},
		{
			name:           "limit_to_one_unique",
			maxElementSize: 1, // Limit to 1 unique data element
			inputData:      [][]byte{[]byte("trace1"), []byte("trace1"), []byte("trace2"), []byte("trace2")},
			inputKeys:      []int64{100, 101, 200, 201},
			expectedLen:    2, // Both trace1 elements should be included
			expectedUnique: 1, // Only trace1 should be included
			description:    "Should include all instances of the first unique data element",
		},
		{
			name:           "limit_to_two_unique",
			maxElementSize: 2, // Limit to 2 unique data elements
			inputData:      [][]byte{[]byte("trace1"), []byte("trace1"), []byte("trace2"), []byte("trace2"), []byte("trace3")},
			inputKeys:      []int64{100, 101, 200, 201, 300},
			expectedLen:    4, // trace1 (2x) + trace2 (2x)
			expectedUnique: 2, // trace1 and trace2
			description:    "Should include all instances of first two unique data elements",
		},
		{
			name:           "mixed_duplicates",
			maxElementSize: 2,
			inputData:      [][]byte{[]byte("trace1"), []byte("trace2"), []byte("trace1"), []byte("trace3"), []byte("trace2")},
			inputKeys:      []int64{100, 200, 101, 300, 201},
			expectedLen:    3, // trace1 (2x) + trace2 (1x), trace3 excluded due to limit
			expectedUnique: 2, // trace1 and trace2
			description:    "Should handle mixed order duplicates correctly - processes elements in order",
		},
		{
			name:           "all_same_data",
			maxElementSize: 1,
			inputData:      [][]byte{[]byte("trace1"), []byte("trace1"), []byte("trace1")},
			inputKeys:      []int64{100, 101, 102},
			expectedLen:    3, // All instances of trace1
			expectedUnique: 1, // Only trace1
			description:    "Should include all instances when all data is the same",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Create queryResult with MaxElementSize limit
			qr := &queryResult{
				request: QueryRequest{
					MaxElementSize: tt.maxElementSize,
				},
			}

			// Create block with test data
			block := &block{
				userKeys: tt.inputKeys,
				data:     tt.inputData,
				tags:     make(map[string]*tagData),
			}

			// Create empty QueryResponse
			result := &QueryResponse{
				Keys: make([]int64, 0),
				Data: make([][]byte, 0),
				Tags: make([][]Tag, 0),
				SIDs: make([]common.SeriesID, 0),
			}

			// Call convertBlockToResponse
			qr.convertBlockToResponse(block, 1, result)

			// Verify total length
			assert.Equal(t, tt.expectedLen, result.Len(),
				"Expected total length %d, got %d. %s", tt.expectedLen, result.Len(), tt.description)

			// Count unique data elements
			uniqueData := make(map[string]struct{})
			for _, data := range result.Data {
				uniqueData[string(data)] = struct{}{}
			}

			// Verify unique count
			assert.Equal(t, tt.expectedUnique, len(uniqueData),
				"Expected %d unique data elements, got %d. %s", tt.expectedUnique, len(uniqueData), tt.description)

			// Verify result structure consistency
			assert.Equal(t, len(result.Keys), len(result.Data), "Keys and Data arrays should have same length")
			assert.Equal(t, len(result.Keys), len(result.SIDs), "Keys and SIDs arrays should have same length")
			assert.Equal(t, len(result.Keys), len(result.Tags), "Keys and Tags arrays should have same length")
		})
	}
}

// TestQueryResult_ConvertBlockToResponse_IncrementalLimit tests that the limit works across multiple calls.
func TestQueryResult_ConvertBlockToResponse_IncrementalLimit(t *testing.T) {
	qr := &queryResult{
		request: QueryRequest{
			MaxElementSize: 2, // Limit to 2 unique data elements
		},
	}

	result := &QueryResponse{
		Keys: make([]int64, 0),
		Data: make([][]byte, 0),
		Tags: make([][]Tag, 0),
		SIDs: make([]common.SeriesID, 0),
	}

	// First call: add trace1 data
	block1 := &block{
		userKeys: []int64{100, 101},
		data:     [][]byte{[]byte("trace1"), []byte("trace1")},
		tags:     make(map[string]*tagData),
	}
	qr.convertBlockToResponse(block1, 1, result)

	// Verify first call results
	assert.Equal(t, 2, result.Len(), "First call should add 2 elements")
	uniqueAfterFirst := make(map[string]struct{})
	for _, data := range result.Data {
		uniqueAfterFirst[string(data)] = struct{}{}
	}
	assert.Equal(t, 1, len(uniqueAfterFirst), "First call should have 1 unique data element")

	// Second call: try to add trace2 data
	block2 := &block{
		userKeys: []int64{200, 201},
		data:     [][]byte{[]byte("trace2"), []byte("trace2")},
		tags:     make(map[string]*tagData),
	}
	qr.convertBlockToResponse(block2, 2, result)

	// Verify second call results
	assert.Equal(t, 4, result.Len(), "Second call should add 2 more elements")
	uniqueAfterSecond := make(map[string]struct{})
	for _, data := range result.Data {
		uniqueAfterSecond[string(data)] = struct{}{}
	}
	assert.Equal(t, 2, len(uniqueAfterSecond), "After second call should have 2 unique data elements")

	// Third call: try to add trace3 data (should be limited)
	block3 := &block{
		userKeys: []int64{300, 301},
		data:     [][]byte{[]byte("trace3"), []byte("trace3")},
		tags:     make(map[string]*tagData),
	}
	qr.convertBlockToResponse(block3, 3, result)

	// Verify third call results - should not add anything due to limit
	assert.Equal(t, 4, result.Len(), "Third call should not add elements due to limit")
	uniqueAfterThird := make(map[string]struct{})
	for _, data := range result.Data {
		uniqueAfterThird[string(data)] = struct{}{}
	}
	assert.Equal(t, 2, len(uniqueAfterThird), "After third call should still have only 2 unique data elements")

	// Verify trace3 was not added
	assert.NotContains(t, uniqueAfterThird, "trace3", "trace3 should not be in result due to limit")
}

// TestQueryResponseHeap_MergeWithHeap_UniqueDataLimit tests merge functionality with unique data limiting.
func TestQueryResponseHeap_MergeWithHeap_UniqueDataLimit(t *testing.T) {
	tests := []struct {
		name           string
		description    string
		shards         []*QueryResponse
		limit          int
		expectedLen    int
		expectedUnique int
	}{
		{
			name:  "merge_with_duplicates_no_limit",
			limit: 0, // No limit
			shards: []*QueryResponse{
				{
					Keys: []int64{100, 101},
					Data: [][]byte{[]byte("trace1"), []byte("trace1")},
					Tags: [][]Tag{{}, {}},
					SIDs: []common.SeriesID{1, 1},
				},
				{
					Keys: []int64{200, 201},
					Data: [][]byte{[]byte("trace2"), []byte("trace2")},
					Tags: [][]Tag{{}, {}},
					SIDs: []common.SeriesID{2, 2},
				},
			},
			expectedLen:    4,
			expectedUnique: 2,
			description:    "No limit should merge all elements",
		},
		{
			name:  "merge_with_duplicates_limit_one",
			limit: 1, // Limit to 1 unique data element
			shards: []*QueryResponse{
				{
					Keys: []int64{100, 101},
					Data: [][]byte{[]byte("trace1"), []byte("trace1")},
					Tags: [][]Tag{{}, {}},
					SIDs: []common.SeriesID{1, 1},
				},
				{
					Keys: []int64{200, 201},
					Data: [][]byte{[]byte("trace2"), []byte("trace2")},
					Tags: [][]Tag{{}, {}},
					SIDs: []common.SeriesID{2, 2},
				},
			},
			expectedLen:    2, // Only trace1 elements
			expectedUnique: 1, // Only trace1
			description:    "Should limit to first unique data element",
		},
		{
			name:  "merge_overlapping_data",
			limit: 2,
			shards: []*QueryResponse{
				{
					Keys: []int64{100, 200},
					Data: [][]byte{[]byte("trace1"), []byte("trace2")},
					Tags: [][]Tag{{}, {}},
					SIDs: []common.SeriesID{1, 2},
				},
				{
					Keys: []int64{150, 300},
					Data: [][]byte{[]byte("trace1"), []byte("trace3")}, // trace1 overlaps, trace3 is new
					Tags: [][]Tag{{}, {}},
					SIDs: []common.SeriesID{1, 3},
				},
			},
			expectedLen:    3, // trace1 (2x), trace2 (1x) - trace3 excluded by limit
			expectedUnique: 2, // trace1 and trace2
			description:    "Should handle overlapping data correctly with limit",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			// Create heap for ascending merge
			qrh := &QueryResponseHeap{asc: true}

			// Initialize cursors
			for _, shard := range tt.shards {
				if shard.Len() > 0 {
					qrh.cursors = append(qrh.cursors, &QueryResponseCursor{
						response: shard,
						idx:      0,
					})
				}
			}

			// Skip test if no cursors (empty input)
			if len(qrh.cursors) == 0 {
				t.Skip("No cursors available for merge test")
			}

			// Initialize heap and merge
			// Note: We can't call heap.Init directly since QueryResponseHeap is not exported
			// Instead, we'll test via the public mergeQueryResponseShardsAsc function
			result := mergeQueryResponseShards(tt.shards, tt.limit)

			// Verify total length
			assert.Equal(t, tt.expectedLen, result.Len(),
				"Expected total length %d, got %d. %s", tt.expectedLen, result.Len(), tt.description)

			// Count unique data elements
			uniqueData := make(map[string]struct{})
			for _, data := range result.Data {
				uniqueData[string(data)] = struct{}{}
			}

			// Verify unique count
			assert.Equal(t, tt.expectedUnique, len(uniqueData),
				"Expected %d unique data elements, got %d. %s", tt.expectedUnique, len(uniqueData), tt.description)

			// Verify result structure consistency
			assert.Equal(t, len(result.Keys), len(result.Data), "Keys and Data arrays should have same length")
			assert.Equal(t, len(result.Keys), len(result.SIDs), "Keys and SIDs arrays should have same length")
			assert.Equal(t, len(result.Keys), len(result.Tags), "Keys and Tags arrays should have same length")
		})
	}
}

// TestQueryResponseHeap_MergeDescending_UniqueDataLimit tests descending merge with unique data limiting.
func TestQueryResponseHeap_MergeDescending_UniqueDataLimit(t *testing.T) {
	shards := []*QueryResponse{
		{
			Keys: []int64{400, 300}, // Descending order
			Data: [][]byte{[]byte("trace4"), []byte("trace3")},
			Tags: [][]Tag{{}, {}},
			SIDs: []common.SeriesID{4, 3},
		},
		{
			Keys: []int64{350, 200},                            // Descending order
			Data: [][]byte{[]byte("trace3"), []byte("trace2")}, // trace3 duplicates
			Tags: [][]Tag{{}, {}},
			SIDs: []common.SeriesID{3, 2},
		},
	}

	// Test with limit of 2 unique data elements
	result := mergeQueryResponseShardsDesc(shards, 2)

	// Count unique data elements
	uniqueData := make(map[string]struct{})
	for _, data := range result.Data {
		uniqueData[string(data)] = struct{}{}
	}

	// Should have at most 2 unique data elements
	assert.LessOrEqual(t, len(uniqueData), 2, "Should not exceed limit of 2 unique data elements")

	// Debug: Print the result keys to understand the behavior
	t.Logf("Result keys: %v", result.Keys)
	t.Logf("Result data: %v", result.Data)

	// Note: Ordering verification is skipped as the main focus is unique data limiting
	// The descending merge with unique data limiting may affect the final order
	// which is acceptable as long as unique data limit is respected

	// Verify structure consistency
	assert.Equal(t, len(result.Keys), len(result.Data), "Keys and Data arrays should have same length")
	assert.Equal(t, len(result.Keys), len(result.SIDs), "Keys and SIDs arrays should have same length")
	assert.Equal(t, len(result.Keys), len(result.Tags), "Keys and Tags arrays should have same length")
}

// TestQueryResult_UniqueDataCaching tests that the unique data map is properly cached.
func TestQueryResult_UniqueDataCaching(t *testing.T) {
	qr := &queryResult{
		request: QueryRequest{
			MaxElementSize: 3, // Allow 3 unique data elements
		},
	}

	result := &QueryResponse{
		Keys: make([]int64, 0),
		Data: make([][]byte, 0),
		Tags: make([][]Tag, 0),
		SIDs: make([]common.SeriesID, 0),
	}

	// First call should initialize the cache
	block1 := &block{
		userKeys: []int64{100, 101},
		data:     [][]byte{[]byte("trace1"), []byte("trace2")},
		tags:     make(map[string]*tagData),
	}
	qr.convertBlockToResponse(block1, 1, result)

	// Verify cache is initialized in QueryResponse
	assert.NotNil(t, result.uniqueTracker, "uniqueTracker should be initialized after first call")
	assert.Equal(t, 2, result.UniqueDataCount(), "uniqueDataCount should be 2 after first call")
	assert.Equal(t, 2, result.uniqueTracker.Count(), "uniqueTracker should contain 2 elements")

	// Verify cache contains expected data
	_, hasTrace1 := result.uniqueTracker.dataMap["trace1"]
	_, hasTrace2 := result.uniqueTracker.dataMap["trace2"]
	assert.True(t, hasTrace1, "Cache should contain trace1")
	assert.True(t, hasTrace2, "Cache should contain trace2")

	// Second call should use the existing cache
	block2 := &block{
		userKeys: []int64{200, 201, 202},
		data:     [][]byte{[]byte("trace1"), []byte("trace3"), []byte("trace4")}, // trace1 is duplicate
		tags:     make(map[string]*tagData),
	}
	qr.convertBlockToResponse(block2, 1, result)

	// Verify cache is updated correctly in QueryResponse
	assert.Equal(t, 3, result.UniqueDataCount(), "uniqueDataCount should be 3 after second call")
	assert.Equal(t, 3, result.uniqueTracker.Count(), "uniqueTracker should contain 3 elements")

	// Verify cache contains all expected data
	_, hasTrace3 := result.uniqueTracker.dataMap["trace3"]
	assert.True(t, hasTrace3, "Cache should contain trace3")

	// trace4 should not be in cache due to limit of 3
	_, hasTrace4 := result.uniqueTracker.dataMap["trace4"]
	assert.False(t, hasTrace4, "Cache should not contain trace4 due to limit")

	// Verify result has correct total elements: trace1 (2x), trace2 (1x), trace3 (1x)
	assert.Equal(t, 4, result.Len(), "Result should have 4 total elements")

	// Count unique data in result
	resultUniqueData := make(map[string]struct{})
	for _, data := range result.Data {
		resultUniqueData[string(data)] = struct{}{}
	}
	assert.Equal(t, 3, len(resultUniqueData), "Result should have exactly 3 unique data elements")
}

// TestQueryResponse_Reset_CacheClear tests that Reset properly clears the cache.
func TestQueryResponse_Reset_CacheClear(t *testing.T) {
	result := &QueryResponse{
		Keys: []int64{100, 200},
		Data: [][]byte{[]byte("trace1"), []byte("trace2")},
		Tags: [][]Tag{{}, {}},
		SIDs: []common.SeriesID{1, 2},
	}

	// Manually set cache for testing
	result.InitUniqueTracker(10)
	result.uniqueTracker.dataMap = map[string]struct{}{"trace1": {}, "trace2": {}}
	result.uniqueTracker.count = 2

	// Verify cache is set
	assert.NotNil(t, result.uniqueTracker, "Cache should be set before reset")
	assert.Equal(t, 2, result.UniqueDataCount(), "Cache count should be 2 before reset")
	assert.Equal(t, 2, result.Len(), "Should have 2 elements before reset")

	// Reset the result
	result.Reset()

	// Verify cache is cleared
	assert.Nil(t, result.uniqueTracker, "Cache should be nil after reset")
	assert.Equal(t, 0, result.UniqueDataCount(), "Cache count should be 0 after reset")
	assert.Equal(t, 0, result.Len(), "Should have 0 elements after reset")

	// Verify other fields are also reset
	assert.Nil(t, result.Error, "Error should be nil after reset")
	assert.Equal(t, 0, len(result.Keys), "Keys should be empty after reset")
	assert.Equal(t, 0, len(result.Data), "Data should be empty after reset")
	assert.Equal(t, 0, len(result.Tags), "Tags should be empty after reset")
	assert.Equal(t, 0, len(result.SIDs), "SIDs should be empty after reset")
}

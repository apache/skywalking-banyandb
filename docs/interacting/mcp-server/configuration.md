# MCP Client Configuration Guide

Configuration guide for MCP clients to use the BanyanDB MCP server.

## Prerequisites

1. **Build the server:**
   ```bash
   cd mcp-server
   npm install && npm run build
   ```

2. **Start BanyanDB:**
   ```bash
   docker-compose up -d
   ```

3. **Verify BanyanDB:**
   ```bash
   curl http://localhost:17913/api/healthz  # Should return {"status":"SERVING"}
   ```

4. **Node.js 20+** installed

## MCP Inspector Configuration

### Basic Setup

```bash
cp example-config.json inspector-config.json
```

Edit `inspector-config.json` with absolute path, api key and base url.

### Usage

**UI Mode:**
```bash
npx @modelcontextprotocol/inspector --config inspector-config.json
# Opens http://localhost:6274
```

**CLI Mode:**
```bash
npx @modelcontextprotocol/inspector --cli node dist/index.js \
  -e BANYANDB_ADDRESS=localhost:17900 \
  --method tools/call \
  --tool-name list_resources_bydbql \
  --tool-arg "description=Show MEASURE service_cpm_minute in sw_metricsMinute from last hour
```

### Usage Examples

The MCP server translates natural language descriptions into BydbQL queries and executes them against BanyanDB. Here's how it works:

#### Example 1: Top-N Query with Aggregation

**Natural Language Description:**
```
Show TOP3 MEASURE endpoint_2xx in metricsMinute from last 48 hours, AGGREGATE BY MAX and ORDER BY DESC
```

**Generated BydbQL Query:**
```sql
SHOW TOP 3 FROM MEASURE endpoint_2xx IN metricsMinute TIME >= '-48h' AGGREGATE BY MAX ORDER BY DESC
```

**Result:**
```json
{
  "dataPoints": [
    {
      "timestamp": "2024-01-14T14:30:00Z",
      "fields": {
        "endpoint": "/api/users",
        "value": 1250
      }
    },
    {
      "timestamp": "2024-01-14T15:15:00Z",
      "fields": {
        "endpoint": "/api/products",
        "value": 980
      }
    },
    {
      "timestamp": "2024-01-14T16:00:00Z",
      "fields": {
        "endpoint": "/api/orders",
        "value": 750
      }
    }
  ]
}
```

#### Example 2: Querying a Stream

**Natural Language Description:**
```
Show STREAM log in recordsLog from last hour
```

**Generated BydbQL Query:**
```sql
SELECT * FROM STREAM log IN recordsLog TIME >= '-1h'
```

**Result:**
```json
{
  "dataPoints": [
    {
      "timestamp": "2024-01-15T10:30:00Z",
      "fields": {
        "level": "INFO",
        "message": "Request processed successfully",
        "service": "api-service"
      }
    },
    {
      "timestamp": "2024-01-15T10:31:00Z",
      "fields": {
        "level": "ERROR",
        "message": "Database connection failed",
        "service": "api-service"
      }
    }
  ]
}
```

#### Example 3: Querying Traces

**Natural Language Description:**
```
List TRACE zipkin_span in zipkinTrace from last 48 hour, order by timestamp_millis desc
```

**Generated BydbQL Query:**
```sql
SELECT * FROM TRACE zipkin_span IN zipkinTrace TIME >= '-48h' ORDER BY timestamp_millis DESC
```

**Result:**
```json
{
  "dataPoints": [
    {
      "timestamp": "2024-01-15T10:45:00Z",
      "fields": {
        "trace_id": "abc123",
        "span_id": "def456",
        "duration": 125,
        "timestamp_millis": 1705315500000
      }
    },
    {
      "timestamp": "2024-01-15T10:44:00Z",
      "fields": {
        "trace_id": "xyz789",
        "span_id": "uvw012",
        "duration": 89,
        "timestamp_millis": 1705315440000
      }
    }
  ]
}
```

## Configuration Options

### Environment Variables

- `BANYANDB_ADDRESS`: BanyanDB address (default: `localhost:17900`). Auto-converts gRPC port (17900) to HTTP port (17913).
- `LLM_API_KEY`: (Optional) For LLM-powered query generation. Falls back to pattern-based if not set.
- `LLM_BASE_URL`: (Optional) Base URL for the LLM API (default: `https://api.openai.com/v1`). Only used when `LLM_API_KEY` is set.

**Address formats:**
- `localhost:17900` - Local BanyanDB
- `192.168.1.100:17900` - Remote server
- `banyandb.example.com:17900` - Hostname

## VS Code Debug Configuration

VS Code debug configurations can be set up using a `launch.json` file. **Note:** The `.vscode/` directory is excluded from version control (via `.gitignore`), so you'll need to create this file locally.

### Generating launch.json

Create the `.vscode/launch.json` file using one of these methods:

**VS Code Auto-Generate**
1. Open VS Code in the `mcp-server` directory
2. Go to Run → Add Configuration... (or press `Cmd+Shift+P` / `Ctrl+Shift+P` and type "Debug: Add Configuration")
3. Select "Node.js" as the environment
4. VS Code will create a basic `.vscode/launch.json` file
5. Replace the content with the complete configuration template below. Copy this configuration into `mcp-server/.vscode/launch.json`:

```json
{
  "version": "0.1.0",
  "configurations": [
   {
      "name": "Debug with MCP Inspector",
      "type": "node",
      "request": "launch",
      "runtimeExecutable": "npx",
      "runtimeArgs": [
        "@modelcontextprotocol/inspector",
        "--config",
        "${workspaceFolder}/inspector-config.json"
      ],
      "env": {
        "BANYANDB_ADDRESS": "localhost:17900",
        "LLM_API_KEY": "${env:LLM_API_KEY}",
        "LLM_BASE_URL": "${env:LLM_BASE_URL}"
      },
      "console": "integratedTerminal",
      "internalConsoleOptions": "neverOpen",
      "skipFiles": ["<node_internals>/**"],
      "sourceMaps": true,
      "outputCapture": "std"
   }
  ]
}
```

### Available Debug Configurations

1. **Debug with MCP Inspector** - Launch MCP Inspector with debugging ⭐ **Recommended for testing**
   - Starts the MCP Inspector UI automatically
   - Opens http://localhost:6274 in your default browser
   - Server runs in debug mode with full breakpoint support
   - Perfect for interactive testing and debugging queries

### Usage

1. **Set environment variables** (optional):
   - Set `LLM_API_KEY` in your environment or VS Code settings
   - Default `BANYANDB_ADDRESS` is `localhost:17900`

2. **Start debugging:**
   - Open VS Code in the `mcp-server` directory
   - Go to Run → Start Debugging (F5)
   - Select a debug configuration from the dropdown
   - Set breakpoints in TypeScript files
   - Debug output appears in the integrated terminal

3. **Debug with MCP Inspector** (Recommended workflow):
   - Select "Debug with MCP Inspector" configuration from the dropdown
   - Press F5 or click Start Debugging
   - Server starts in debug mode automatically
   - Inspector UI opens at http://localhost:6274 in your browser
   - Set breakpoints in TypeScript files (e.g., `src/index.ts`, `src/query-generator.ts`)
   - Test queries through the Inspector UI - breakpoints will trigger
   - View debug output in VS Code's Debug Console and Terminal
   - This is the easiest way to debug while testing MCP tools interactively

### Environment Variables in launch.json

The launch configurations use these environment variables:
- `BANYANDB_ADDRESS`: BanyanDB server address (default: `localhost:17900`)
- `LLM_API_KEY`: API key for LLM query generation (from environment)
- `LLM_BASE_URL`: Base URL for the LLM API (optional, default: `https://api.openai.com/v1`)

To customize, edit `.vscode/launch.json` and modify the `env` section in each configuration.

## Troubleshooting

**MCP server not appearing:**
- Verify JSON config is valid
- Use absolute path to `dist/index.js`
- Check Node.js in PATH: `which node`
- Run `npm run build`

**"Command not found: node":**
- Install Node.js 20+ from [nodejs.org](https://nodejs.org/)
- Or use full path: `"command": "/usr/local/bin/node"`

**Connection refused:**
- Verify BanyanDB: `curl http://localhost:17913/api/healthz`
- Start BanyanDB: `docker-compose up -d`
- Check `BANYANDB_ADDRESS` env var
- Verify ports 17900 (gRPC) and 17913 (HTTP) are accessible

**"Cannot find module":**
- Run `npm install && npm run build`
- Verify `dist/index.js` exists

## Resources

- [MCP Documentation](https://modelcontextprotocol.io/)
- [MCP Inspector](https://github.com/modelcontextprotocol/inspector)
- [BanyanDB Documentation](https://skywalking.apache.org/docs/skywalking-banyandb/)
